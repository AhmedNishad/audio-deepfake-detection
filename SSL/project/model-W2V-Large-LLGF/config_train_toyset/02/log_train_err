D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-03-11 22:44:26 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\torch\nn\utils\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.
  warnings.warn("torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.")
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\torch\nn\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)
  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
72,LA_T_5894355,0,64000,0, 1/118, Time: 0.380491s, Loss: 0.529122
70,LA_T_5883755,0,64000,0, 2/118, Time: 0.380491s, Loss: 0.529122
90,LA_T_7124444,1,21405,64000, 3/118, Time: 0.380491s, Loss: 0.529122
43,LA_T_3467377,1,26131,64000, 4/118, Time: 0.380491s, Loss: 0.529122
34,LA_T_3021659,1,10227,64000, 5/118, Time: 0.380491s, Loss: 0.529122
2,LA_T_1346935,0,45741,0, 6/118, Time: 0.380491s, Loss: 0.529122
76,LA_T_6331069,1,64000,64000, 7/118, Time: 0.380491s, Loss: 0.529122
38,LA_T_3220265,0,38149,0, 8/118, Time: 0.380491s, Loss: 0.529122
36,LA_T_3189628,0,64000,0, 9/118, Time: 0.380491s, Loss: 0.529122
37,LA_T_3189628,1,15851,64000, 10/118, Time: 0.380491s, Loss: 0.529122
8,LA_T_1589546,0,64000,0, 11/118, Time: 0.380491s, Loss: 0.529122
98,LA_T_7422011,0,45646,0, 12/118, Time: 0.380491s, Loss: 0.529122
80,LA_T_6487227,0,44111,0, 13/118, Time: 0.380491s, Loss: 0.529122
102,LA_T_7808689,0,64000,0, 14/118, Time: 0.380491s, Loss: 0.529122
99,LA_T_7529623,0,43309,0, 15/118, Time: 0.380491s, Loss: 0.529122
106,LA_T_8090857,0,64000,0, 16/118, Time: 0.380491s, Loss: 0.529122
86,LA_T_6941395,1,29307,64000, 17/118, Time: 0.380491s, Loss: 0.529122
93,LA_T_7281115,0,64000,0, 18/118, Time: 0.380491s, Loss: 0.529122
47,LA_T_3627265,0,64000,0, 19/118, Time: 0.380491s, Loss: 0.529122
87,LA_T_6960076,0,42296,0, 20/118, Time: 0.380491s, Loss: 0.529122
24,LA_T_2327393,0,60701,0, 21/118, Time: 0.380491s, Loss: 0.529122
71,LA_T_5883755,1,19926,64000, 22/118, Time: 0.380491s, Loss: 0.529122
10,LA_T_1630611,0,58109,0, 23/118, Time: 0.380491s, Loss: 0.529122
82,LA_T_6728875,0,64000,0, 24/118, Time: 0.380491s, Loss: 0.529122
115,LA_T_9746209,0,64000,0, 25/118, Time: 0.380491s, Loss: 0.529122
52,LA_T_4096754,0,31273,0, 26/118, Time: 0.380491s, Loss: 0.529122
22,LA_T_2277153,1,21822,64000, 27/118, Time: 0.380491s, Loss: 0.529122
83,LA_T_6751630,0,62290,0, 28/118, Time: 0.380491s, Loss: 0.529122
104,LA_T_7888797,0,26626,0, 29/118, Time: 0.380491s, Loss: 0.529122
92,LA_T_7273305,0,53173,0, 30/118, Time: 0.380491s, Loss: 0.529122
110,LA_T_8681550,0,40836,0, 31/118, Time: 0.380491s, Loss: 0.529122
50,LA_T_3935691,0,39156,0, 32/118, Time: 0.380491s, Loss: 0.529122
39,LA_T_3250806,0,61271,0, 33/118, Time: 0.380491s, Loss: 0.529122
116,LA_T_9746209,1,16967,64000, 34/118, Time: 0.380491s, Loss: 0.529122
88,LA_T_7020532,0,59863,0, 35/118, Time: 0.380491s, Loss: 0.529122
54,LA_T_4584407,0,36921,0, 36/118, Time: 0.380491s, Loss: 0.529122
112,LA_T_9540683,0,25430,0, 37/118, Time: 0.380491s, Loss: 0.529122
44,LA_T_3527643,0,64000,0, 38/118, Time: 0.380491s, Loss: 0.529122
32,LA_T_2995069,0,49112,0, 39/118, Time: 0.380491s, Loss: 0.529122
45,LA_T_3527643,1,8295,64000, 40/118, Time: 0.380491s, Loss: 0.529122
53,LA_T_4512109,0,51563,0, 41/118, Time: 0.380491s, Loss: 0.529122
41,LA_T_3391018,0,57402,0, 42/118, Time: 0.380491s, Loss: 0.529122
28,LA_T_2419035,1,10625,64000, 43/118, Time: 0.380491s, Loss: 0.529122
1,LA_T_1258641,0,44562,0, 44/118, Time: 0.380491s, Loss: 0.529122
114,LA_T_9668514,0,54564,0, 45/118, Time: 0.380491s, Loss: 0.529122
20,LA_T_2269038,0,63442,0, 46/118, Time: 0.380491s, Loss: 0.529122
107,LA_T_8206162,0,55328,0, 47/118, Time: 0.380491s, Loss: 0.529122
64,LA_T_5258800,0,54370,0, 48/118, Time: 0.380491s, Loss: 0.529122
77,LA_T_6390981,0,64000,0, 49/118, Time: 0.380491s, Loss: 0.529122
81,LA_T_6521388,0,64000,0, 50/118, Time: 0.380491s, Loss: 0.529122
11,LA_T_1653822,0,64000,0, 51/118, Time: 0.380491s, Loss: 0.529122
89,LA_T_7124444,0,64000,0, 52/118, Time: 0.380491s, Loss: 0.529122
84,LA_T_6822716,0,27313,0, 53/118, Time: 0.380491s, Loss: 0.529122
7,LA_T_1518499,0,29284,0, 54/118, Time: 0.380491s, Loss: 0.529122
61,LA_T_5209704,0,64000,0, 55/118, Time: 0.380491s, Loss: 0.529122
0,LA_T_1154440,0,30107,0, 56/118, Time: 0.380491s, Loss: 0.529122
66,LA_T_5433188,0,34172,0, 57/118, Time: 0.380491s, Loss: 0.529122
73,LA_T_5978739,0,33363,0, 58/118, Time: 0.380491s, Loss: 0.529122
94,LA_T_7281115,1,11690,64000, 59/118, Time: 0.380491s, Loss: 0.529122
46,LA_T_3588714,0,46298,0, 60/118, Time: 0.380491s, Loss: 0.529122
59,LA_T_5015679,0,41144,0, 61/118, Time: 0.380491s, Loss: 0.529122
75,LA_T_6331069,0,64000,0, 62/118, Time: 0.380491s, Loss: 0.529122
21,LA_T_2277153,0,64000,0, 63/118, Time: 0.380491s, Loss: 0.529122
109,LA_T_8527420,0,63377,0, 64/118, Time: 0.380491s, Loss: 0.529122
103,LA_T_7866363,0,63473,0, 65/118, Time: 0.492261s, Loss: 0.390342
58,LA_T_4928920,0,64000,0, 66/118, Time: 0.492261s, Loss: 0.390342
13,LA_T_1724943,0,60906,0, 67/118, Time: 0.492261s, Loss: 0.390342
26,LA_T_2354581,0,47687,0, 68/118, Time: 0.492261s, Loss: 0.390342
65,LA_T_5360018,0,54554,0, 69/118, Time: 0.492261s, Loss: 0.390342
111,LA_T_9226984,0,45529,0, 70/118, Time: 0.492261s, Loss: 0.390342
117,LA_T_9830298,0,64000,0, 71/118, Time: 0.492261s, Loss: 0.390342
27,LA_T_2419035,0,64000,0, 72/118, Time: 0.492261s, Loss: 0.390342
105,LA_T_8007992,0,45465,0, 73/118, Time: 0.492261s, Loss: 0.390342
55,LA_T_4630359,0,64000,0, 74/118, Time: 0.492261s, Loss: 0.390342
30,LA_T_2584761,0,47107,0, 75/118, Time: 0.492261s, Loss: 0.390342
57,LA_T_4774044,0,59379,0, 76/118, Time: 0.492261s, Loss: 0.390342
14,LA_T_1836557,0,43646,0, 77/118, Time: 0.492261s, Loss: 0.390342
69,LA_T_5786445,0,64000,0, 78/118, Time: 0.492261s, Loss: 0.390342
108,LA_T_8234484,0,46183,0, 79/118, Time: 0.492261s, Loss: 0.390342
5,LA_T_1486451,0,50406,0, 80/118, Time: 0.492261s, Loss: 0.390342
3,LA_T_1373588,0,46949,0, 81/118, Time: 0.492261s, Loss: 0.390342
4,LA_T_1470918,0,39641,0, 82/118, Time: 0.492261s, Loss: 0.390342
68,LA_T_5522733,0,61381,0, 83/118, Time: 0.492261s, Loss: 0.390342
67,LA_T_5492534,0,28365,0, 84/118, Time: 0.492261s, Loss: 0.390342
62,LA_T_5209704,1,34511,64000, 85/118, Time: 0.492261s, Loss: 0.390342
51,LA_T_3993688,0,48709,0, 86/118, Time: 0.492261s, Loss: 0.390342
78,LA_T_6435787,0,43614,0, 87/118, Time: 0.492261s, Loss: 0.390342
23,LA_T_2320617,0,56846,0, 88/118, Time: 0.492261s, Loss: 0.390342
17,LA_T_1909651,1,64000,64000, 89/118, Time: 0.492261s, Loss: 0.390342
79,LA_T_6476207,0,54948,0, 90/118, Time: 0.492261s, Loss: 0.390342
12,LA_T_1653822,1,9756,64000, 91/118, Time: 0.492261s, Loss: 0.390342
100,LA_T_7550620,0,44981,0, 92/118, Time: 0.492261s, Loss: 0.390342
16,LA_T_1909651,0,64000,0, 93/118, Time: 0.492261s, Loss: 0.390342
74,LA_T_6301739,0,41843,0, 94/118, Time: 0.492261s, Loss: 0.390342
60,LA_T_5205025,0,61550,0, 95/118, Time: 0.492261s, Loss: 0.390342
25,LA_T_2333843,0,61708,0, 96/118, Time: 0.492261s, Loss: 0.390342
96,LA_T_7359423,0,50164,0, 97/118, Time: 0.492261s, Loss: 0.390342
49,LA_T_3758169,0,46047,0, 98/118, Time: 0.492261s, Loss: 0.390342
6,LA_T_1490244,0,55572,0, 99/118, Time: 0.492261s, Loss: 0.390342
19,LA_T_2052267,0,38415,0, 100/118, Time: 0.492261s, Loss: 0.390342
15,LA_T_1870524,0,43190,0, 101/118, Time: 0.492261s, Loss: 0.390342
113,LA_T_9633872,0,27407,0, 102/118, Time: 0.492261s, Loss: 0.390342
95,LA_T_7314513,0,39437,0, 103/118, Time: 0.492261s, Loss: 0.390342
40,LA_T_3319380,0,42032,0, 104/118, Time: 0.492261s, Loss: 0.390342
85,LA_T_6941395,0,64000,0, 105/118, Time: 0.492261s, Loss: 0.390342
56,LA_T_4725126,0,61141,0, 106/118, Time: 0.492261s, Loss: 0.390342
29,LA_T_2422854,0,62661,0, 107/118, Time: 0.492261s, Loss: 0.390342
63,LA_T_5214047,0,41920,0, 108/118, Time: 0.492261s, Loss: 0.390342
91,LA_T_7223887,0,40210,0, 109/118, Time: 0.492261s, Loss: 0.390342
97,LA_T_7405543,0,36914,0, 110/118, Time: 0.492261s, Loss: 0.390342
42,LA_T_3467377,0,64000,0, 111/118, Time: 0.492261s, Loss: 0.390342
48,LA_T_3627265,1,44402,64000, 112/118, Time: 0.492261s, Loss: 0.390342
18,LA_T_1909651,2,24481,128000, 113/118, Time: 0.492261s, Loss: 0.390342
31,LA_T_2677562,0,63649,0, 114/118, Time: 0.492261s, Loss: 0.390342
101,LA_T_7706110,0,24083,0, 115/118, Time: 0.492261s, Loss: 0.390342
9,LA_T_1589546,1,9736,64000, 116/118, Time: 0.492261s, Loss: 0.390342
35,LA_T_3176741,0,62198,0, 117/118, Time: 0.492261s, Loss: 0.390342
33,LA_T_3021659,0,64000,0, 118/118, Time: 0.492261s, Loss: 0.390342
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Uni\FYP\Implementation\Code\SSL\core_scripts\data_io\customize_collate_fn.py:114: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = elem.storage()._new_shared(numel)
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Uni\FYP\Implementation\Code\SSL\core_scripts\data_io\customize_collate_fn.py:114: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = elem.storage()._new_shared(numel)
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
19,LA_D_4013191,0,34443,0, 1/63, Time: 0.268550s, Loss: 0.300022
9,LA_D_2082512,0,53318,0, 2/63, Time: 0.268550s, Loss: 0.300022
27,LA_D_5239066,1,30014,64000, 3/63, Time: 0.268550s, Loss: 0.300022
49,LA_D_7933136,0,64000,0, 4/63, Time: 0.268550s, Loss: 0.300022
2,LA_D_1317561,0,64000,0, 5/63, Time: 0.268550s, Loss: 0.300022
25,LA_D_4630224,0,27639,0, 6/63, Time: 0.268550s, Loss: 0.300022
26,LA_D_5239066,0,64000,0, 7/63, Time: 0.268550s, Loss: 0.300022
44,LA_D_7095518,0,56482,0, 8/63, Time: 0.268550s, Loss: 0.300022
48,LA_D_7869978,1,20551,64000, 9/63, Time: 0.268550s, Loss: 0.300022
11,LA_D_2648941,1,41414,64000, 10/63, Time: 0.268550s, Loss: 0.300022
57,LA_D_9493396,0,40904,0, 11/63, Time: 0.268550s, Loss: 0.300022
55,LA_D_9192205,0,40612,0, 12/63, Time: 0.268550s, Loss: 0.300022
1,LA_D_1179848,1,29541,64000, 13/63, Time: 0.268550s, Loss: 0.300022
45,LA_D_7452217,0,25016,0, 14/63, Time: 0.268550s, Loss: 0.300022
10,LA_D_2648941,0,64000,0, 15/63, Time: 0.268550s, Loss: 0.300022
3,LA_D_1317561,1,8976,64000, 16/63, Time: 0.268550s, Loss: 0.300022
32,LA_D_5505879,1,35600,64000, 17/63, Time: 0.268550s, Loss: 0.300022
42,LA_D_6502788,0,29166,0, 18/63, Time: 0.268550s, Loss: 0.300022
46,LA_D_7862876,0,57311,0, 19/63, Time: 0.268550s, Loss: 0.300022
4,LA_D_1366945,0,45986,0, 20/63, Time: 0.268550s, Loss: 0.300022
50,LA_D_7974256,0,50504,0, 21/63, Time: 0.268550s, Loss: 0.300022
39,LA_D_5944972,1,39788,64000, 22/63, Time: 0.268550s, Loss: 0.300022
51,LA_D_8228250,0,42344,0, 23/63, Time: 0.268550s, Loss: 0.300022
20,LA_D_4265541,0,26061,0, 24/63, Time: 0.268550s, Loss: 0.300022
34,LA_D_5741681,0,44990,0, 25/63, Time: 0.268550s, Loss: 0.300022
14,LA_D_2949136,1,27301,64000, 26/63, Time: 0.268550s, Loss: 0.300022
41,LA_D_6180779,0,23109,0, 27/63, Time: 0.268550s, Loss: 0.300022
29,LA_D_5300881,1,27489,64000, 28/63, Time: 0.268550s, Loss: 0.300022
33,LA_D_5659407,0,64000,0, 29/63, Time: 0.268550s, Loss: 0.300022
43,LA_D_7026375,0,61915,0, 30/63, Time: 0.268550s, Loss: 0.300022
24,LA_D_4519635,1,12487,64000, 31/63, Time: 0.268550s, Loss: 0.300022
58,LA_D_9566347,0,35349,0, 32/63, Time: 0.268550s, Loss: 0.300022
23,LA_D_4519635,0,64000,0, 33/63, Time: 0.268550s, Loss: 0.300022
8,LA_D_1886178,0,40092,0, 34/63, Time: 0.268550s, Loss: 0.300022
35,LA_D_5835948,0,64000,0, 35/63, Time: 0.268550s, Loss: 0.300022
13,LA_D_2949136,0,64000,0, 36/63, Time: 0.268550s, Loss: 0.300022
16,LA_D_3457616,0,51111,0, 37/63, Time: 0.268550s, Loss: 0.300022
38,LA_D_5944972,0,64000,0, 38/63, Time: 0.268550s, Loss: 0.300022
17,LA_D_3705418,0,64000,0, 39/63, Time: 0.268550s, Loss: 0.300022
60,LA_D_9686838,1,28544,64000, 40/63, Time: 0.268550s, Loss: 0.300022
59,LA_D_9686838,0,64000,0, 41/63, Time: 0.268550s, Loss: 0.300022
7,LA_D_1803008,0,45707,0, 42/63, Time: 0.268550s, Loss: 0.300022
53,LA_D_8584336,0,61424,0, 43/63, Time: 0.268550s, Loss: 0.300022
56,LA_D_9316963,0,58699,0, 44/63, Time: 0.268550s, Loss: 0.300022
15,LA_D_3203408,0,49607,0, 45/63, Time: 0.268550s, Loss: 0.300022
0,LA_D_1179848,0,64000,0, 46/63, Time: 0.268550s, Loss: 0.300022
22,LA_D_4394367,0,61722,0, 47/63, Time: 0.268550s, Loss: 0.300022
54,LA_D_8998984,0,37498,0, 48/63, Time: 0.268550s, Loss: 0.300022
18,LA_D_3983088,0,21649,0, 49/63, Time: 0.268550s, Loss: 0.300022
40,LA_D_6055606,0,53176,0, 50/63, Time: 0.268550s, Loss: 0.300022
30,LA_D_5441528,0,24518,0, 51/63, Time: 0.268550s, Loss: 0.300022
12,LA_D_2896709,0,64000,0, 52/63, Time: 0.268550s, Loss: 0.300022
62,LA_D_9753761,1,22997,64000, 53/63, Time: 0.268550s, Loss: 0.300022
31,LA_D_5505879,0,64000,0, 54/63, Time: 0.268550s, Loss: 0.300022
52,LA_D_8284460,0,50174,0, 55/63, Time: 0.268550s, Loss: 0.300022
36,LA_D_5835948,1,29523,64000, 56/63, Time: 0.268550s, Loss: 0.300022
28,LA_D_5300881,0,64000,0, 57/63, Time: 0.268550s, Loss: 0.300022
61,LA_D_9753761,0,64000,0, 58/63, Time: 0.268550s, Loss: 0.300022
21,LA_D_4276413,0,49755,0, 59/63, Time: 0.268550s, Loss: 0.300022
37,LA_D_5891869,0,54006,0, 60/63, Time: 0.268550s, Loss: 0.300022
47,LA_D_7869978,0,64000,0, 61/63, Time: 0.268550s, Loss: 0.300022
5,LA_D_1435765,0,47544,0, 62/63, Time: 0.268550s, Loss: 0.300022
6,LA_D_1580841,0,38838,0, 63/63, Time: 0.268550s, Loss: 0.300022
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Uni\FYP\Implementation\Code\SSL\core_scripts\data_io\customize_collate_fn.py:114: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = elem.storage()._new_shared(numel)
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
101,LA_T_7706110,0,24083,0, 1/118, Time: 0.541154s, Loss: 0.367166
109,LA_T_8527420,0,63377,0, 2/118, Time: 0.541154s, Loss: 0.367166
115,LA_T_9746209,0,64000,0, 3/118, Time: 0.541154s, Loss: 0.367166
14,LA_T_1836557,0,43646,0, 4/118, Time: 0.541154s, Loss: 0.367166
75,LA_T_6331069,0,64000,0, 5/118, Time: 0.541154s, Loss: 0.367166
67,LA_T_5492534,0,28365,0, 6/118, Time: 0.541154s, Loss: 0.367166
90,LA_T_7124444,1,21405,64000, 7/118, Time: 0.541154s, Loss: 0.367166
64,LA_T_5258800,0,54370,0, 8/118, Time: 0.541154s, Loss: 0.367166
69,LA_T_5786445,0,64000,0, 9/118, Time: 0.541154s, Loss: 0.367166
62,LA_T_5209704,1,34511,64000, 10/118, Time: 0.541154s, Loss: 0.367166
13,LA_T_1724943,0,60906,0, 11/118, Time: 0.541154s, Loss: 0.367166
9,LA_T_1589546,1,9736,64000, 12/118, Time: 0.541154s, Loss: 0.367166
28,LA_T_2419035,1,10625,64000, 13/118, Time: 0.541154s, Loss: 0.367166
92,LA_T_7273305,0,53173,0, 14/118, Time: 0.541154s, Loss: 0.367166
22,LA_T_2277153,1,21822,64000, 15/118, Time: 0.541154s, Loss: 0.367166
60,LA_T_5205025,0,61550,0, 16/118, Time: 0.541154s, Loss: 0.367166
26,LA_T_2354581,0,47687,0, 17/118, Time: 0.541154s, Loss: 0.367166
59,LA_T_5015679,0,41144,0, 18/118, Time: 0.541154s, Loss: 0.367166
5,LA_T_1486451,0,50406,0, 19/118, Time: 0.541154s, Loss: 0.367166
49,LA_T_3758169,0,46047,0, 20/118, Time: 0.541154s, Loss: 0.367166
108,LA_T_8234484,0,46183,0, 21/118, Time: 0.541154s, Loss: 0.367166
17,LA_T_1909651,1,64000,64000, 22/118, Time: 0.541154s, Loss: 0.367166
24,LA_T_2327393,0,60701,0, 23/118, Time: 0.541154s, Loss: 0.367166
55,LA_T_4630359,0,64000,0, 24/118, Time: 0.541154s, Loss: 0.367166
97,LA_T_7405543,0,36914,0, 25/118, Time: 0.541154s, Loss: 0.367166
81,LA_T_6521388,0,64000,0, 26/118, Time: 0.541154s, Loss: 0.367166
16,LA_T_1909651,0,64000,0, 27/118, Time: 0.541154s, Loss: 0.367166
80,LA_T_6487227,0,44111,0, 28/118, Time: 0.541154s, Loss: 0.367166
77,LA_T_6390981,0,64000,0, 29/118, Time: 0.541154s, Loss: 0.367166
74,LA_T_6301739,0,41843,0, 30/118, Time: 0.541154s, Loss: 0.367166
53,LA_T_4512109,0,51563,0, 31/118, Time: 0.541154s, Loss: 0.367166
113,LA_T_9633872,0,27407,0, 32/118, Time: 0.541154s, Loss: 0.367166
105,LA_T_8007992,0,45465,0, 33/118, Time: 0.541154s, Loss: 0.367166
86,LA_T_6941395,1,29307,64000, 34/118, Time: 0.541154s, Loss: 0.367166
3,LA_T_1373588,0,46949,0, 35/118, Time: 0.541154s, Loss: 0.367166
100,LA_T_7550620,0,44981,0, 36/118, Time: 0.541154s, Loss: 0.367166
85,LA_T_6941395,0,64000,0, 37/118, Time: 0.541154s, Loss: 0.367166
114,LA_T_9668514,0,54564,0, 38/118, Time: 0.541154s, Loss: 0.367166
42,LA_T_3467377,0,64000,0, 39/118, Time: 0.541154s, Loss: 0.367166
34,LA_T_3021659,1,10227,64000, 40/118, Time: 0.541154s, Loss: 0.367166
41,LA_T_3391018,0,57402,0, 41/118, Time: 0.541154s, Loss: 0.367166
23,LA_T_2320617,0,56846,0, 42/118, Time: 0.541154s, Loss: 0.367166
0,LA_T_1154440,0,30107,0, 43/118, Time: 0.541154s, Loss: 0.367166
7,LA_T_1518499,0,29284,0, 44/118, Time: 0.541154s, Loss: 0.367166
43,LA_T_3467377,1,26131,64000, 45/118, Time: 0.541154s, Loss: 0.367166
110,LA_T_8681550,0,40836,0, 46/118, Time: 0.541154s, Loss: 0.367166
52,LA_T_4096754,0,31273,0, 47/118, Time: 0.541154s, Loss: 0.367166
58,LA_T_4928920,0,64000,0, 48/118, Time: 0.541154s, Loss: 0.367166
104,LA_T_7888797,0,26626,0, 49/118, Time: 0.541154s, Loss: 0.367166
40,LA_T_3319380,0,42032,0, 50/118, Time: 0.541154s, Loss: 0.367166
45,LA_T_3527643,1,8295,64000, 51/118, Time: 0.541154s, Loss: 0.367166
2,LA_T_1346935,0,45741,0, 52/118, Time: 0.541154s, Loss: 0.367166
15,LA_T_1870524,0,43190,0, 53/118, Time: 0.541154s, Loss: 0.367166
84,LA_T_6822716,0,27313,0, 54/118, Time: 0.541154s, Loss: 0.367166
111,LA_T_9226984,0,45529,0, 55/118, Time: 0.541154s, Loss: 0.367166
25,LA_T_2333843,0,61708,0, 56/118, Time: 0.541154s, Loss: 0.367166
63,LA_T_5214047,0,41920,0, 57/118, Time: 0.541154s, Loss: 0.367166
116,LA_T_9746209,1,16967,64000, 58/118, Time: 0.541154s, Loss: 0.367166
102,LA_T_7808689,0,64000,0, 59/118, Time: 0.541154s, Loss: 0.367166
47,LA_T_3627265,0,64000,0, 60/118, Time: 0.541154s, Loss: 0.367166
65,LA_T_5360018,0,54554,0, 61/118, Time: 0.541154s, Loss: 0.367166
54,LA_T_4584407,0,36921,0, 62/118, Time: 0.541154s, Loss: 0.367166
107,LA_T_8206162,0,55328,0, 63/118, Time: 0.541154s, Loss: 0.367166
6,LA_T_1490244,0,55572,0, 64/118, Time: 0.541154s, Loss: 0.367166
96,LA_T_7359423,0,50164,0, 65/118, Time: 0.545409s, Loss: 0.311146
94,LA_T_7281115,1,11690,64000, 66/118, Time: 0.545409s, Loss: 0.311146
37,LA_T_3189628,1,15851,64000, 67/118, Time: 0.545409s, Loss: 0.311146
48,LA_T_3627265,1,44402,64000, 68/118, Time: 0.545409s, Loss: 0.311146
88,LA_T_7020532,0,59863,0, 69/118, Time: 0.545409s, Loss: 0.311146
35,LA_T_3176741,0,62198,0, 70/118, Time: 0.545409s, Loss: 0.311146
87,LA_T_6960076,0,42296,0, 71/118, Time: 0.545409s, Loss: 0.311146
44,LA_T_3527643,0,64000,0, 72/118, Time: 0.545409s, Loss: 0.311146
30,LA_T_2584761,0,47107,0, 73/118, Time: 0.545409s, Loss: 0.311146
36,LA_T_3189628,0,64000,0, 74/118, Time: 0.545409s, Loss: 0.311146
46,LA_T_3588714,0,46298,0, 75/118, Time: 0.545409s, Loss: 0.311146
19,LA_T_2052267,0,38415,0, 76/118, Time: 0.545409s, Loss: 0.311146
72,LA_T_5894355,0,64000,0, 77/118, Time: 0.545409s, Loss: 0.311146
33,LA_T_3021659,0,64000,0, 78/118, Time: 0.545409s, Loss: 0.311146
93,LA_T_7281115,0,64000,0, 79/118, Time: 0.545409s, Loss: 0.311146
106,LA_T_8090857,0,64000,0, 80/118, Time: 0.545409s, Loss: 0.311146
61,LA_T_5209704,0,64000,0, 81/118, Time: 0.545409s, Loss: 0.311146
71,LA_T_5883755,1,19926,64000, 82/118, Time: 0.545409s, Loss: 0.311146
10,LA_T_1630611,0,58109,0, 83/118, Time: 0.545409s, Loss: 0.311146
29,LA_T_2422854,0,62661,0, 84/118, Time: 0.545409s, Loss: 0.311146
103,LA_T_7866363,0,63473,0, 85/118, Time: 0.545409s, Loss: 0.311146
18,LA_T_1909651,2,24481,128000, 86/118, Time: 0.545409s, Loss: 0.311146
66,LA_T_5433188,0,34172,0, 87/118, Time: 0.545409s, Loss: 0.311146
89,LA_T_7124444,0,64000,0, 88/118, Time: 0.545409s, Loss: 0.311146
39,LA_T_3250806,0,61271,0, 89/118, Time: 0.545409s, Loss: 0.311146
91,LA_T_7223887,0,40210,0, 90/118, Time: 0.545409s, Loss: 0.311146
82,LA_T_6728875,0,64000,0, 91/118, Time: 0.545409s, Loss: 0.311146
73,LA_T_5978739,0,33363,0, 92/118, Time: 0.545409s, Loss: 0.311146
50,LA_T_3935691,0,39156,0, 93/118, Time: 0.545409s, Loss: 0.311146
95,LA_T_7314513,0,39437,0, 94/118, Time: 0.545409s, Loss: 0.311146
12,LA_T_1653822,1,9756,64000, 95/118, Time: 0.545409s, Loss: 0.311146
78,LA_T_6435787,0,43614,0, 96/118, Time: 0.545409s, Loss: 0.311146
51,LA_T_3993688,0,48709,0, 97/118, Time: 0.545409s, Loss: 0.311146
79,LA_T_6476207,0,54948,0, 98/118, Time: 0.545409s, Loss: 0.311146
4,LA_T_1470918,0,39641,0, 99/118, Time: 0.545409s, Loss: 0.311146
1,LA_T_1258641,0,44562,0, 100/118, Time: 0.545409s, Loss: 0.311146
70,LA_T_5883755,0,64000,0, 101/118, Time: 0.545409s, Loss: 0.311146
56,LA_T_4725126,0,61141,0, 102/118, Time: 0.545409s, Loss: 0.311146
11,LA_T_1653822,0,64000,0, 103/118, Time: 0.545409s, Loss: 0.311146
99,LA_T_7529623,0,43309,0, 104/118, Time: 0.545409s, Loss: 0.311146
32,LA_T_2995069,0,49112,0, 105/118, Time: 0.545409s, Loss: 0.311146
27,LA_T_2419035,0,64000,0, 106/118, Time: 0.545409s, Loss: 0.311146
68,LA_T_5522733,0,61381,0, 107/118, Time: 0.545409s, Loss: 0.311146
117,LA_T_9830298,0,64000,0, 108/118, Time: 0.545409s, Loss: 0.311146
20,LA_T_2269038,0,63442,0, 109/118, Time: 0.545409s, Loss: 0.311146
83,LA_T_6751630,0,62290,0, 110/118, Time: 0.545409s, Loss: 0.311146
21,LA_T_2277153,0,64000,0, 111/118, Time: 0.545409s, Loss: 0.311146
112,LA_T_9540683,0,25430,0, 112/118, Time: 0.545409s, Loss: 0.311146
8,LA_T_1589546,0,64000,0, 113/118, Time: 0.545409s, Loss: 0.311146
31,LA_T_2677562,0,63649,0, 114/118, Time: 0.545409s, Loss: 0.311146
76,LA_T_6331069,1,64000,64000, 115/118, Time: 0.545409s, Loss: 0.311146
57,LA_T_4774044,0,59379,0, 116/118, Time: 0.545409s, Loss: 0.311146
38,LA_T_3220265,0,38149,0, 117/118, Time: 0.545409s, Loss: 0.311146
98,LA_T_7422011,0,45646,0, 118/118, Time: 0.545409s, Loss: 0.311146
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Uni\FYP\Implementation\Code\SSL\core_scripts\data_io\customize_collate_fn.py:114: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = elem.storage()._new_shared(numel)
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Uni\FYP\Implementation\Code\SSL\core_scripts\data_io\customize_collate_fn.py:114: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = elem.storage()._new_shared(numel)
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
3,LA_D_1317561,1,8976,64000, 1/63, Time: 0.364018s, Loss: 0.290751
32,LA_D_5505879,1,35600,64000, 2/63, Time: 0.364018s, Loss: 0.290751
18,LA_D_3983088,0,21649,0, 3/63, Time: 0.364018s, Loss: 0.290751
11,LA_D_2648941,1,41414,64000, 4/63, Time: 0.364018s, Loss: 0.290751
8,LA_D_1886178,0,40092,0, 5/63, Time: 0.364018s, Loss: 0.290751
33,LA_D_5659407,0,64000,0, 6/63, Time: 0.364018s, Loss: 0.290751
29,LA_D_5300881,1,27489,64000, 7/63, Time: 0.364018s, Loss: 0.290751
6,LA_D_1580841,0,38838,0, 8/63, Time: 0.364018s, Loss: 0.290751
17,LA_D_3705418,0,64000,0, 9/63, Time: 0.364018s, Loss: 0.290751
44,LA_D_7095518,0,56482,0, 10/63, Time: 0.364018s, Loss: 0.290751
39,LA_D_5944972,1,39788,64000, 11/63, Time: 0.364018s, Loss: 0.290751
58,LA_D_9566347,0,35349,0, 12/63, Time: 0.364018s, Loss: 0.290751
0,LA_D_1179848,0,64000,0, 13/63, Time: 0.364018s, Loss: 0.290751
16,LA_D_3457616,0,51111,0, 14/63, Time: 0.364018s, Loss: 0.290751
37,LA_D_5891869,0,54006,0, 15/63, Time: 0.364018s, Loss: 0.290751
2,LA_D_1317561,0,64000,0, 16/63, Time: 0.364018s, Loss: 0.290751
43,LA_D_7026375,0,61915,0, 17/63, Time: 0.364018s, Loss: 0.290751
57,LA_D_9493396,0,40904,0, 18/63, Time: 0.364018s, Loss: 0.290751
14,LA_D_2949136,1,27301,64000, 19/63, Time: 0.364018s, Loss: 0.290751
22,LA_D_4394367,0,61722,0, 20/63, Time: 0.364018s, Loss: 0.290751
51,LA_D_8228250,0,42344,0, 21/63, Time: 0.364018s, Loss: 0.290751
40,LA_D_6055606,0,53176,0, 22/63, Time: 0.364018s, Loss: 0.290751
7,LA_D_1803008,0,45707,0, 23/63, Time: 0.364018s, Loss: 0.290751
4,LA_D_1366945,0,45986,0, 24/63, Time: 0.364018s, Loss: 0.290751
30,LA_D_5441528,0,24518,0, 25/63, Time: 0.364018s, Loss: 0.290751
49,LA_D_7933136,0,64000,0, 26/63, Time: 0.364018s, Loss: 0.290751
41,LA_D_6180779,0,23109,0, 27/63, Time: 0.364018s, Loss: 0.290751
25,LA_D_4630224,0,27639,0, 28/63, Time: 0.364018s, Loss: 0.290751
52,LA_D_8284460,0,50174,0, 29/63, Time: 0.364018s, Loss: 0.290751
62,LA_D_9753761,1,22997,64000, 30/63, Time: 0.364018s, Loss: 0.290751
1,LA_D_1179848,1,29541,64000, 31/63, Time: 0.364018s, Loss: 0.290751
21,LA_D_4276413,0,49755,0, 32/63, Time: 0.364018s, Loss: 0.290751
23,LA_D_4519635,0,64000,0, 33/63, Time: 0.364018s, Loss: 0.290751
15,LA_D_3203408,0,49607,0, 34/63, Time: 0.364018s, Loss: 0.290751
38,LA_D_5944972,0,64000,0, 35/63, Time: 0.364018s, Loss: 0.290751
24,LA_D_4519635,1,12487,64000, 36/63, Time: 0.364018s, Loss: 0.290751
59,LA_D_9686838,0,64000,0, 37/63, Time: 0.364018s, Loss: 0.290751
48,LA_D_7869978,1,20551,64000, 38/63, Time: 0.364018s, Loss: 0.290751
56,LA_D_9316963,0,58699,0, 39/63, Time: 0.364018s, Loss: 0.290751
35,LA_D_5835948,0,64000,0, 40/63, Time: 0.364018s, Loss: 0.290751
60,LA_D_9686838,1,28544,64000, 41/63, Time: 0.364018s, Loss: 0.290751
20,LA_D_4265541,0,26061,0, 42/63, Time: 0.364018s, Loss: 0.290751
12,LA_D_2896709,0,64000,0, 43/63, Time: 0.364018s, Loss: 0.290751
9,LA_D_2082512,0,53318,0, 44/63, Time: 0.364018s, Loss: 0.290751
13,LA_D_2949136,0,64000,0, 45/63, Time: 0.364018s, Loss: 0.290751
55,LA_D_9192205,0,40612,0, 46/63, Time: 0.364018s, Loss: 0.290751
5,LA_D_1435765,0,47544,0, 47/63, Time: 0.364018s, Loss: 0.290751
10,LA_D_2648941,0,64000,0, 48/63, Time: 0.364018s, Loss: 0.290751
45,LA_D_7452217,0,25016,0, 49/63, Time: 0.364018s, Loss: 0.290751
54,LA_D_8998984,0,37498,0, 50/63, Time: 0.364018s, Loss: 0.290751
50,LA_D_7974256,0,50504,0, 51/63, Time: 0.364018s, Loss: 0.290751
27,LA_D_5239066,1,30014,64000, 52/63, Time: 0.364018s, Loss: 0.290751
46,LA_D_7862876,0,57311,0, 53/63, Time: 0.364018s, Loss: 0.290751
47,LA_D_7869978,0,64000,0, 54/63, Time: 0.364018s, Loss: 0.290751
34,LA_D_5741681,0,44990,0, 55/63, Time: 0.364018s, Loss: 0.290751
61,LA_D_9753761,0,64000,0, 56/63, Time: 0.364018s, Loss: 0.290751
26,LA_D_5239066,0,64000,0, 57/63, Time: 0.364018s, Loss: 0.290751
31,LA_D_5505879,0,64000,0, 58/63, Time: 0.364018s, Loss: 0.290751
36,LA_D_5835948,1,29523,64000, 59/63, Time: 0.364018s, Loss: 0.290751
42,LA_D_6502788,0,29166,0, 60/63, Time: 0.364018s, Loss: 0.290751
19,LA_D_4013191,0,34443,0, 61/63, Time: 0.364018s, Loss: 0.290751
53,LA_D_8584336,0,61424,0, 62/63, Time: 0.364018s, Loss: 0.290751
28,LA_D_5300881,0,64000,0, 63/63, Time: 0.364018s, Loss: 0.290751
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Uni\FYP\Implementation\Code\SSL\core_scripts\data_io\customize_collate_fn.py:114: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = elem.storage()._new_shared(numel)
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
52,LA_T_4096754,0,31273,0, 1/118, Time: 0.556496s, Loss: 0.300279
23,LA_T_2320617,0,56846,0, 2/118, Time: 0.556496s, Loss: 0.300279
86,LA_T_6941395,1,29307,64000, 3/118, Time: 0.556496s, Loss: 0.300279
55,LA_T_4630359,0,64000,0, 4/118, Time: 0.556496s, Loss: 0.300279
26,LA_T_2354581,0,47687,0, 5/118, Time: 0.556496s, Loss: 0.300279
112,LA_T_9540683,0,25430,0, 6/118, Time: 0.556496s, Loss: 0.300279
69,LA_T_5786445,0,64000,0, 7/118, Time: 0.556496s, Loss: 0.300279
67,LA_T_5492534,0,28365,0, 8/118, Time: 0.556496s, Loss: 0.300279
8,LA_T_1589546,0,64000,0, 9/118, Time: 0.556496s, Loss: 0.300279
76,LA_T_6331069,1,64000,64000, 10/118, Time: 0.556496s, Loss: 0.300279
106,LA_T_8090857,0,64000,0, 11/118, Time: 0.556496s, Loss: 0.300279
104,LA_T_7888797,0,26626,0, 12/118, Time: 0.556496s, Loss: 0.300279
38,LA_T_3220265,0,38149,0, 13/118, Time: 0.556496s, Loss: 0.300279
98,LA_T_7422011,0,45646,0, 14/118, Time: 0.556496s, Loss: 0.300279
92,LA_T_7273305,0,53173,0, 15/118, Time: 0.556496s, Loss: 0.300279
80,LA_T_6487227,0,44111,0, 16/118, Time: 0.556496s, Loss: 0.300279
34,LA_T_3021659,1,10227,64000, 17/118, Time: 0.556496s, Loss: 0.300279
36,LA_T_3189628,0,64000,0, 18/118, Time: 0.556496s, Loss: 0.300279
47,LA_T_3627265,0,64000,0, 19/118, Time: 0.556496s, Loss: 0.300279
5,LA_T_1486451,0,50406,0, 20/118, Time: 0.556496s, Loss: 0.300279
13,LA_T_1724943,0,60906,0, 21/118, Time: 0.556496s, Loss: 0.300279
14,LA_T_1836557,0,43646,0, 22/118, Time: 0.556496s, Loss: 0.300279
115,LA_T_9746209,0,64000,0, 23/118, Time: 0.556496s, Loss: 0.300279
32,LA_T_2995069,0,49112,0, 24/118, Time: 0.556496s, Loss: 0.300279
40,LA_T_3319380,0,42032,0, 25/118, Time: 0.556496s, Loss: 0.300279
101,LA_T_7706110,0,24083,0, 26/118, Time: 0.556496s, Loss: 0.300279
71,LA_T_5883755,1,19926,64000, 27/118, Time: 0.556496s, Loss: 0.300279
25,LA_T_2333843,0,61708,0, 28/118, Time: 0.556496s, Loss: 0.300279
15,LA_T_1870524,0,43190,0, 29/118, Time: 0.556496s, Loss: 0.300279
41,LA_T_3391018,0,57402,0, 30/118, Time: 0.556496s, Loss: 0.300279
11,LA_T_1653822,0,64000,0, 31/118, Time: 0.556496s, Loss: 0.300279
53,LA_T_4512109,0,51563,0, 32/118, Time: 0.556496s, Loss: 0.300279
16,LA_T_1909651,0,64000,0, 33/118, Time: 0.556496s, Loss: 0.300279
65,LA_T_5360018,0,54554,0, 34/118, Time: 0.556496s, Loss: 0.300279
3,LA_T_1373588,0,46949,0, 35/118, Time: 0.556496s, Loss: 0.300279
110,LA_T_8681550,0,40836,0, 36/118, Time: 0.556496s, Loss: 0.300279
97,LA_T_7405543,0,36914,0, 37/118, Time: 0.556496s, Loss: 0.300279
79,LA_T_6476207,0,54948,0, 38/118, Time: 0.556496s, Loss: 0.300279
45,LA_T_3527643,1,8295,64000, 39/118, Time: 0.556496s, Loss: 0.300279
29,LA_T_2422854,0,62661,0, 40/118, Time: 0.556496s, Loss: 0.300279
77,LA_T_6390981,0,64000,0, 41/118, Time: 0.556496s, Loss: 0.300279
73,LA_T_5978739,0,33363,0, 42/118, Time: 0.556496s, Loss: 0.300279
33,LA_T_3021659,0,64000,0, 43/118, Time: 0.556496s, Loss: 0.300279
21,LA_T_2277153,0,64000,0, 44/118, Time: 0.556496s, Loss: 0.300279
100,LA_T_7550620,0,44981,0, 45/118, Time: 0.556496s, Loss: 0.300279
85,LA_T_6941395,0,64000,0, 46/118, Time: 0.556496s, Loss: 0.300279
6,LA_T_1490244,0,55572,0, 47/118, Time: 0.556496s, Loss: 0.300279
4,LA_T_1470918,0,39641,0, 48/118, Time: 0.556496s, Loss: 0.300279
18,LA_T_1909651,2,24481,128000, 49/118, Time: 0.556496s, Loss: 0.300279
59,LA_T_5015679,0,41144,0, 50/118, Time: 0.556496s, Loss: 0.300279
75,LA_T_6331069,0,64000,0, 51/118, Time: 0.556496s, Loss: 0.300279
20,LA_T_2269038,0,63442,0, 52/118, Time: 0.556496s, Loss: 0.300279
42,LA_T_3467377,0,64000,0, 53/118, Time: 0.556496s, Loss: 0.300279
109,LA_T_8527420,0,63377,0, 54/118, Time: 0.556496s, Loss: 0.300279
84,LA_T_6822716,0,27313,0, 55/118, Time: 0.556496s, Loss: 0.300279
27,LA_T_2419035,0,64000,0, 56/118, Time: 0.556496s, Loss: 0.300279
62,LA_T_5209704,1,34511,64000, 57/118, Time: 0.556496s, Loss: 0.300279
103,LA_T_7866363,0,63473,0, 58/118, Time: 0.556496s, Loss: 0.300279
54,LA_T_4584407,0,36921,0, 59/118, Time: 0.556496s, Loss: 0.300279
48,LA_T_3627265,1,44402,64000, 60/118, Time: 0.556496s, Loss: 0.300279
51,LA_T_3993688,0,48709,0, 61/118, Time: 0.556496s, Loss: 0.300279
46,LA_T_3588714,0,46298,0, 62/118, Time: 0.556496s, Loss: 0.300279
70,LA_T_5883755,0,64000,0, 63/118, Time: 0.556496s, Loss: 0.300279
116,LA_T_9746209,1,16967,64000, 64/118, Time: 0.556496s, Loss: 0.300279
1,LA_T_1258641,0,44562,0, 65/118, Time: 0.543988s, Loss: 0.325360
105,LA_T_8007992,0,45465,0, 66/118, Time: 0.543988s, Loss: 0.325360
90,LA_T_7124444,1,21405,64000, 67/118, Time: 0.543988s, Loss: 0.325360
96,LA_T_7359423,0,50164,0, 68/118, Time: 0.543988s, Loss: 0.325360
113,LA_T_9633872,0,27407,0, 69/118, Time: 0.543988s, Loss: 0.325360
37,LA_T_3189628,1,15851,64000, 70/118, Time: 0.543988s, Loss: 0.325360
35,LA_T_3176741,0,62198,0, 71/118, Time: 0.543988s, Loss: 0.325360
50,LA_T_3935691,0,39156,0, 72/118, Time: 0.543988s, Loss: 0.325360
81,LA_T_6521388,0,64000,0, 73/118, Time: 0.543988s, Loss: 0.325360
30,LA_T_2584761,0,47107,0, 74/118, Time: 0.543988s, Loss: 0.325360
10,LA_T_1630611,0,58109,0, 75/118, Time: 0.543988s, Loss: 0.325360
24,LA_T_2327393,0,60701,0, 76/118, Time: 0.543988s, Loss: 0.325360
19,LA_T_2052267,0,38415,0, 77/118, Time: 0.543988s, Loss: 0.325360
7,LA_T_1518499,0,29284,0, 78/118, Time: 0.543988s, Loss: 0.325360
94,LA_T_7281115,1,11690,64000, 79/118, Time: 0.543988s, Loss: 0.325360
64,LA_T_5258800,0,54370,0, 80/118, Time: 0.543988s, Loss: 0.325360
93,LA_T_7281115,0,64000,0, 81/118, Time: 0.543988s, Loss: 0.325360
28,LA_T_2419035,1,10625,64000, 82/118, Time: 0.543988s, Loss: 0.325360
83,LA_T_6751630,0,62290,0, 83/118, Time: 0.543988s, Loss: 0.325360
68,LA_T_5522733,0,61381,0, 84/118, Time: 0.543988s, Loss: 0.325360
57,LA_T_4774044,0,59379,0, 85/118, Time: 0.543988s, Loss: 0.325360
95,LA_T_7314513,0,39437,0, 86/118, Time: 0.543988s, Loss: 0.325360
0,LA_T_1154440,0,30107,0, 87/118, Time: 0.543988s, Loss: 0.325360
82,LA_T_6728875,0,64000,0, 88/118, Time: 0.543988s, Loss: 0.325360
60,LA_T_5205025,0,61550,0, 89/118, Time: 0.543988s, Loss: 0.325360
78,LA_T_6435787,0,43614,0, 90/118, Time: 0.543988s, Loss: 0.325360
89,LA_T_7124444,0,64000,0, 91/118, Time: 0.543988s, Loss: 0.325360
9,LA_T_1589546,1,9736,64000, 92/118, Time: 0.543988s, Loss: 0.325360
102,LA_T_7808689,0,64000,0, 93/118, Time: 0.543988s, Loss: 0.325360
2,LA_T_1346935,0,45741,0, 94/118, Time: 0.543988s, Loss: 0.325360
107,LA_T_8206162,0,55328,0, 95/118, Time: 0.543988s, Loss: 0.325360
91,LA_T_7223887,0,40210,0, 96/118, Time: 0.543988s, Loss: 0.325360
88,LA_T_7020532,0,59863,0, 97/118, Time: 0.543988s, Loss: 0.325360
49,LA_T_3758169,0,46047,0, 98/118, Time: 0.543988s, Loss: 0.325360
74,LA_T_6301739,0,41843,0, 99/118, Time: 0.543988s, Loss: 0.325360
87,LA_T_6960076,0,42296,0, 100/118, Time: 0.543988s, Loss: 0.325360
111,LA_T_9226984,0,45529,0, 101/118, Time: 0.543988s, Loss: 0.325360
12,LA_T_1653822,1,9756,64000, 102/118, Time: 0.543988s, Loss: 0.325360
117,LA_T_9830298,0,64000,0, 103/118, Time: 0.543988s, Loss: 0.325360
61,LA_T_5209704,0,64000,0, 104/118, Time: 0.543988s, Loss: 0.325360
114,LA_T_9668514,0,54564,0, 105/118, Time: 0.543988s, Loss: 0.325360
66,LA_T_5433188,0,34172,0, 106/118, Time: 0.543988s, Loss: 0.325360
31,LA_T_2677562,0,63649,0, 107/118, Time: 0.543988s, Loss: 0.325360
43,LA_T_3467377,1,26131,64000, 108/118, Time: 0.543988s, Loss: 0.325360
39,LA_T_3250806,0,61271,0, 109/118, Time: 0.543988s, Loss: 0.325360
72,LA_T_5894355,0,64000,0, 110/118, Time: 0.543988s, Loss: 0.325360
63,LA_T_5214047,0,41920,0, 111/118, Time: 0.543988s, Loss: 0.325360
56,LA_T_4725126,0,61141,0, 112/118, Time: 0.543988s, Loss: 0.325360
108,LA_T_8234484,0,46183,0, 113/118, Time: 0.543988s, Loss: 0.325360
99,LA_T_7529623,0,43309,0, 114/118, Time: 0.543988s, Loss: 0.325360
58,LA_T_4928920,0,64000,0, 115/118, Time: 0.543988s, Loss: 0.325360
17,LA_T_1909651,1,64000,64000, 116/118, Time: 0.543988s, Loss: 0.325360
44,LA_T_3527643,0,64000,0, 117/118, Time: 0.543988s, Loss: 0.325360
22,LA_T_2277153,1,21822,64000, 118/118, Time: 0.543988s, Loss: 0.325360
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Uni\FYP\Implementation\Code\SSL\core_scripts\data_io\customize_collate_fn.py:114: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = elem.storage()._new_shared(numel)
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Uni\FYP\Implementation\Code\SSL\core_scripts\data_io\customize_collate_fn.py:114: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = elem.storage()._new_shared(numel)
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
10,LA_D_2648941,0,64000,0, 1/63, Time: 0.247953s, Loss: 0.298745
22,LA_D_4394367,0,61722,0, 2/63, Time: 0.247953s, Loss: 0.298745
47,LA_D_7869978,0,64000,0, 3/63, Time: 0.247953s, Loss: 0.298745
55,LA_D_9192205,0,40612,0, 4/63, Time: 0.247953s, Loss: 0.298745
21,LA_D_4276413,0,49755,0, 5/63, Time: 0.247953s, Loss: 0.298745
39,LA_D_5944972,1,39788,64000, 6/63, Time: 0.247953s, Loss: 0.298745
31,LA_D_5505879,0,64000,0, 7/63, Time: 0.247953s, Loss: 0.298745
37,LA_D_5891869,0,54006,0, 8/63, Time: 0.247953s, Loss: 0.298745
40,LA_D_6055606,0,53176,0, 9/63, Time: 0.247953s, Loss: 0.298745
29,LA_D_5300881,1,27489,64000, 10/63, Time: 0.247953s, Loss: 0.298745
1,LA_D_1179848,1,29541,64000, 11/63, Time: 0.247953s, Loss: 0.298745
35,LA_D_5835948,0,64000,0, 12/63, Time: 0.247953s, Loss: 0.298745
2,LA_D_1317561,0,64000,0, 13/63, Time: 0.247953s, Loss: 0.298745
50,LA_D_7974256,0,50504,0, 14/63, Time: 0.247953s, Loss: 0.298745
57,LA_D_9493396,0,40904,0, 15/63, Time: 0.247953s, Loss: 0.298745
44,LA_D_7095518,0,56482,0, 16/63, Time: 0.247953s, Loss: 0.298745
15,LA_D_3203408,0,49607,0, 17/63, Time: 0.247953s, Loss: 0.298745
60,LA_D_9686838,1,28544,64000, 18/63, Time: 0.247953s, Loss: 0.298745
12,LA_D_2896709,0,64000,0, 19/63, Time: 0.247953s, Loss: 0.298745
51,LA_D_8228250,0,42344,0, 20/63, Time: 0.247953s, Loss: 0.298745
8,LA_D_1886178,0,40092,0, 21/63, Time: 0.247953s, Loss: 0.298745
5,LA_D_1435765,0,47544,0, 22/63, Time: 0.247953s, Loss: 0.298745
28,LA_D_5300881,0,64000,0, 23/63, Time: 0.247953s, Loss: 0.298745
59,LA_D_9686838,0,64000,0, 24/63, Time: 0.247953s, Loss: 0.298745
41,LA_D_6180779,0,23109,0, 25/63, Time: 0.247953s, Loss: 0.298745
53,LA_D_8584336,0,61424,0, 26/63, Time: 0.247953s, Loss: 0.298745
42,LA_D_6502788,0,29166,0, 27/63, Time: 0.247953s, Loss: 0.298745
32,LA_D_5505879,1,35600,64000, 28/63, Time: 0.247953s, Loss: 0.298745
49,LA_D_7933136,0,64000,0, 29/63, Time: 0.247953s, Loss: 0.298745
58,LA_D_9566347,0,35349,0, 30/63, Time: 0.247953s, Loss: 0.298745
4,LA_D_1366945,0,45986,0, 31/63, Time: 0.247953s, Loss: 0.298745
26,LA_D_5239066,0,64000,0, 32/63, Time: 0.247953s, Loss: 0.298745
62,LA_D_9753761,1,22997,64000, 33/63, Time: 0.247953s, Loss: 0.298745
25,LA_D_4630224,0,27639,0, 34/63, Time: 0.247953s, Loss: 0.298745
48,LA_D_7869978,1,20551,64000, 35/63, Time: 0.247953s, Loss: 0.298745
54,LA_D_8998984,0,37498,0, 36/63, Time: 0.247953s, Loss: 0.298745
6,LA_D_1580841,0,38838,0, 37/63, Time: 0.247953s, Loss: 0.298745
33,LA_D_5659407,0,64000,0, 38/63, Time: 0.247953s, Loss: 0.298745
24,LA_D_4519635,1,12487,64000, 39/63, Time: 0.247953s, Loss: 0.298745
19,LA_D_4013191,0,34443,0, 40/63, Time: 0.247953s, Loss: 0.298745
38,LA_D_5944972,0,64000,0, 41/63, Time: 0.247953s, Loss: 0.298745
17,LA_D_3705418,0,64000,0, 42/63, Time: 0.247953s, Loss: 0.298745
30,LA_D_5441528,0,24518,0, 43/63, Time: 0.247953s, Loss: 0.298745
11,LA_D_2648941,1,41414,64000, 44/63, Time: 0.247953s, Loss: 0.298745
3,LA_D_1317561,1,8976,64000, 45/63, Time: 0.247953s, Loss: 0.298745
56,LA_D_9316963,0,58699,0, 46/63, Time: 0.247953s, Loss: 0.298745
23,LA_D_4519635,0,64000,0, 47/63, Time: 0.247953s, Loss: 0.298745
14,LA_D_2949136,1,27301,64000, 48/63, Time: 0.247953s, Loss: 0.298745
61,LA_D_9753761,0,64000,0, 49/63, Time: 0.247953s, Loss: 0.298745
43,LA_D_7026375,0,61915,0, 50/63, Time: 0.247953s, Loss: 0.298745
34,LA_D_5741681,0,44990,0, 51/63, Time: 0.247953s, Loss: 0.298745
27,LA_D_5239066,1,30014,64000, 52/63, Time: 0.247953s, Loss: 0.298745
13,LA_D_2949136,0,64000,0, 53/63, Time: 0.247953s, Loss: 0.298745
0,LA_D_1179848,0,64000,0, 54/63, Time: 0.247953s, Loss: 0.298745
45,LA_D_7452217,0,25016,0, 55/63, Time: 0.247953s, Loss: 0.298745
46,LA_D_7862876,0,57311,0, 56/63, Time: 0.247953s, Loss: 0.298745
16,LA_D_3457616,0,51111,0, 57/63, Time: 0.247953s, Loss: 0.298745
36,LA_D_5835948,1,29523,64000, 58/63, Time: 0.247953s, Loss: 0.298745
9,LA_D_2082512,0,53318,0, 59/63, Time: 0.247953s, Loss: 0.298745
18,LA_D_3983088,0,21649,0, 60/63, Time: 0.247953s, Loss: 0.298745
7,LA_D_1803008,0,45707,0, 61/63, Time: 0.247953s, Loss: 0.298745
52,LA_D_8284460,0,50174,0, 62/63, Time: 0.247953s, Loss: 0.298745
20,LA_D_4265541,0,26061,0, 63/63, Time: 0.247953s, Loss: 0.298745
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Uni\FYP\Implementation\Code\SSL\core_scripts\data_io\customize_collate_fn.py:114: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = elem.storage()._new_shared(numel)
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
36,LA_T_3189628,0,64000,0, 1/118, Time: 0.500916s, Loss: 0.402027
65,LA_T_5360018,0,54554,0, 2/118, Time: 0.500916s, Loss: 0.402027
35,LA_T_3176741,0,62198,0, 3/118, Time: 0.500916s, Loss: 0.402027
43,LA_T_3467377,1,26131,64000, 4/118, Time: 0.500916s, Loss: 0.402027
14,LA_T_1836557,0,43646,0, 5/118, Time: 0.500916s, Loss: 0.402027
22,LA_T_2277153,1,21822,64000, 6/118, Time: 0.500916s, Loss: 0.402027
59,LA_T_5015679,0,41144,0, 7/118, Time: 0.500916s, Loss: 0.402027
9,LA_T_1589546,1,9736,64000, 8/118, Time: 0.500916s, Loss: 0.402027
68,LA_T_5522733,0,61381,0, 9/118, Time: 0.500916s, Loss: 0.402027
40,LA_T_3319380,0,42032,0, 10/118, Time: 0.500916s, Loss: 0.402027
13,LA_T_1724943,0,60906,0, 11/118, Time: 0.500916s, Loss: 0.402027
1,LA_T_1258641,0,44562,0, 12/118, Time: 0.500916s, Loss: 0.402027
7,LA_T_1518499,0,29284,0, 13/118, Time: 0.500916s, Loss: 0.402027
54,LA_T_4584407,0,36921,0, 14/118, Time: 0.500916s, Loss: 0.402027
38,LA_T_3220265,0,38149,0, 15/118, Time: 0.500916s, Loss: 0.402027
33,LA_T_3021659,0,64000,0, 16/118, Time: 0.500916s, Loss: 0.402027
89,LA_T_7124444,0,64000,0, 17/118, Time: 0.500916s, Loss: 0.402027
116,LA_T_9746209,1,16967,64000, 18/118, Time: 0.500916s, Loss: 0.402027
64,LA_T_5258800,0,54370,0, 19/118, Time: 0.500916s, Loss: 0.402027
101,LA_T_7706110,0,24083,0, 20/118, Time: 0.500916s, Loss: 0.402027
96,LA_T_7359423,0,50164,0, 21/118, Time: 0.500916s, Loss: 0.402027
114,LA_T_9668514,0,54564,0, 22/118, Time: 0.500916s, Loss: 0.402027
95,LA_T_7314513,0,39437,0, 23/118, Time: 0.500916s, Loss: 0.402027
113,LA_T_9633872,0,27407,0, 24/118, Time: 0.500916s, Loss: 0.402027
12,LA_T_1653822,1,9756,64000, 25/118, Time: 0.500916s, Loss: 0.402027
41,LA_T_3391018,0,57402,0, 26/118, Time: 0.500916s, Loss: 0.402027
100,LA_T_7550620,0,44981,0, 27/118, Time: 0.500916s, Loss: 0.402027
102,LA_T_7808689,0,64000,0, 28/118, Time: 0.500916s, Loss: 0.402027
29,LA_T_2422854,0,62661,0, 29/118, Time: 0.500916s, Loss: 0.402027
0,LA_T_1154440,0,30107,0, 30/118, Time: 0.500916s, Loss: 0.402027
115,LA_T_9746209,0,64000,0, 31/118, Time: 0.500916s, Loss: 0.402027
105,LA_T_8007992,0,45465,0, 32/118, Time: 0.500916s, Loss: 0.402027
31,LA_T_2677562,0,63649,0, 33/118, Time: 0.500916s, Loss: 0.402027
81,LA_T_6521388,0,64000,0, 34/118, Time: 0.500916s, Loss: 0.402027
5,LA_T_1486451,0,50406,0, 35/118, Time: 0.500916s, Loss: 0.402027
6,LA_T_1490244,0,55572,0, 36/118, Time: 0.500916s, Loss: 0.402027
56,LA_T_4725126,0,61141,0, 37/118, Time: 0.500916s, Loss: 0.402027
26,LA_T_2354581,0,47687,0, 38/118, Time: 0.500916s, Loss: 0.402027
98,LA_T_7422011,0,45646,0, 39/118, Time: 0.500916s, Loss: 0.402027
37,LA_T_3189628,1,15851,64000, 40/118, Time: 0.500916s, Loss: 0.402027
27,LA_T_2419035,0,64000,0, 41/118, Time: 0.500916s, Loss: 0.402027
73,LA_T_5978739,0,33363,0, 42/118, Time: 0.500916s, Loss: 0.402027
50,LA_T_3935691,0,39156,0, 43/118, Time: 0.500916s, Loss: 0.402027
19,LA_T_2052267,0,38415,0, 44/118, Time: 0.500916s, Loss: 0.402027
78,LA_T_6435787,0,43614,0, 45/118, Time: 0.500916s, Loss: 0.402027
53,LA_T_4512109,0,51563,0, 46/118, Time: 0.500916s, Loss: 0.402027
92,LA_T_7273305,0,53173,0, 47/118, Time: 0.500916s, Loss: 0.402027
94,LA_T_7281115,1,11690,64000, 48/118, Time: 0.500916s, Loss: 0.402027
66,LA_T_5433188,0,34172,0, 49/118, Time: 0.500916s, Loss: 0.402027
16,LA_T_1909651,0,64000,0, 50/118, Time: 0.500916s, Loss: 0.402027
104,LA_T_7888797,0,26626,0, 51/118, Time: 0.500916s, Loss: 0.402027
85,LA_T_6941395,0,64000,0, 52/118, Time: 0.500916s, Loss: 0.402027
103,LA_T_7866363,0,63473,0, 53/118, Time: 0.500916s, Loss: 0.402027
117,LA_T_9830298,0,64000,0, 54/118, Time: 0.500916s, Loss: 0.402027
8,LA_T_1589546,0,64000,0, 55/118, Time: 0.500916s, Loss: 0.402027
45,LA_T_3527643,1,8295,64000, 56/118, Time: 0.500916s, Loss: 0.402027
25,LA_T_2333843,0,61708,0, 57/118, Time: 0.500916s, Loss: 0.402027
47,LA_T_3627265,0,64000,0, 58/118, Time: 0.500916s, Loss: 0.402027
42,LA_T_3467377,0,64000,0, 59/118, Time: 0.500916s, Loss: 0.402027
91,LA_T_7223887,0,40210,0, 60/118, Time: 0.500916s, Loss: 0.402027
63,LA_T_5214047,0,41920,0, 61/118, Time: 0.500916s, Loss: 0.402027
90,LA_T_7124444,1,21405,64000, 62/118, Time: 0.500916s, Loss: 0.402027
72,LA_T_5894355,0,64000,0, 63/118, Time: 0.500916s, Loss: 0.402027
69,LA_T_5786445,0,64000,0, 64/118, Time: 0.500916s, Loss: 0.402027
52,LA_T_4096754,0,31273,0, 65/118, Time: 0.551577s, Loss: 0.272075
21,LA_T_2277153,0,64000,0, 66/118, Time: 0.551577s, Loss: 0.272075
86,LA_T_6941395,1,29307,64000, 67/118, Time: 0.551577s, Loss: 0.272075
61,LA_T_5209704,0,64000,0, 68/118, Time: 0.551577s, Loss: 0.272075
48,LA_T_3627265,1,44402,64000, 69/118, Time: 0.551577s, Loss: 0.272075
57,LA_T_4774044,0,59379,0, 70/118, Time: 0.551577s, Loss: 0.272075
58,LA_T_4928920,0,64000,0, 71/118, Time: 0.551577s, Loss: 0.272075
112,LA_T_9540683,0,25430,0, 72/118, Time: 0.551577s, Loss: 0.272075
111,LA_T_9226984,0,45529,0, 73/118, Time: 0.551577s, Loss: 0.272075
51,LA_T_3993688,0,48709,0, 74/118, Time: 0.551577s, Loss: 0.272075
109,LA_T_8527420,0,63377,0, 75/118, Time: 0.551577s, Loss: 0.272075
93,LA_T_7281115,0,64000,0, 76/118, Time: 0.551577s, Loss: 0.272075
17,LA_T_1909651,1,64000,64000, 77/118, Time: 0.551577s, Loss: 0.272075
28,LA_T_2419035,1,10625,64000, 78/118, Time: 0.551577s, Loss: 0.272075
18,LA_T_1909651,2,24481,128000, 79/118, Time: 0.551577s, Loss: 0.272075
34,LA_T_3021659,1,10227,64000, 80/118, Time: 0.551577s, Loss: 0.272075
24,LA_T_2327393,0,60701,0, 81/118, Time: 0.551577s, Loss: 0.272075
110,LA_T_8681550,0,40836,0, 82/118, Time: 0.551577s, Loss: 0.272075
55,LA_T_4630359,0,64000,0, 83/118, Time: 0.551577s, Loss: 0.272075
60,LA_T_5205025,0,61550,0, 84/118, Time: 0.551577s, Loss: 0.272075
88,LA_T_7020532,0,59863,0, 85/118, Time: 0.551577s, Loss: 0.272075
83,LA_T_6751630,0,62290,0, 86/118, Time: 0.551577s, Loss: 0.272075
39,LA_T_3250806,0,61271,0, 87/118, Time: 0.551577s, Loss: 0.272075
106,LA_T_8090857,0,64000,0, 88/118, Time: 0.551577s, Loss: 0.272075
99,LA_T_7529623,0,43309,0, 89/118, Time: 0.551577s, Loss: 0.272075
71,LA_T_5883755,1,19926,64000, 90/118, Time: 0.551577s, Loss: 0.272075
77,LA_T_6390981,0,64000,0, 91/118, Time: 0.551577s, Loss: 0.272075
82,LA_T_6728875,0,64000,0, 92/118, Time: 0.551577s, Loss: 0.272075
76,LA_T_6331069,1,64000,64000, 93/118, Time: 0.551577s, Loss: 0.272075
67,LA_T_5492534,0,28365,0, 94/118, Time: 0.551577s, Loss: 0.272075
2,LA_T_1346935,0,45741,0, 95/118, Time: 0.551577s, Loss: 0.272075
49,LA_T_3758169,0,46047,0, 96/118, Time: 0.551577s, Loss: 0.272075
23,LA_T_2320617,0,56846,0, 97/118, Time: 0.551577s, Loss: 0.272075
44,LA_T_3527643,0,64000,0, 98/118, Time: 0.551577s, Loss: 0.272075
108,LA_T_8234484,0,46183,0, 99/118, Time: 0.551577s, Loss: 0.272075
75,LA_T_6331069,0,64000,0, 100/118, Time: 0.551577s, Loss: 0.272075
97,LA_T_7405543,0,36914,0, 101/118, Time: 0.551577s, Loss: 0.272075
107,LA_T_8206162,0,55328,0, 102/118, Time: 0.551577s, Loss: 0.272075
15,LA_T_1870524,0,43190,0, 103/118, Time: 0.551577s, Loss: 0.272075
62,LA_T_5209704,1,34511,64000, 104/118, Time: 0.551577s, Loss: 0.272075
11,LA_T_1653822,0,64000,0, 105/118, Time: 0.551577s, Loss: 0.272075
30,LA_T_2584761,0,47107,0, 106/118, Time: 0.551577s, Loss: 0.272075
4,LA_T_1470918,0,39641,0, 107/118, Time: 0.551577s, Loss: 0.272075
74,LA_T_6301739,0,41843,0, 108/118, Time: 0.551577s, Loss: 0.272075
80,LA_T_6487227,0,44111,0, 109/118, Time: 0.551577s, Loss: 0.272075
79,LA_T_6476207,0,54948,0, 110/118, Time: 0.551577s, Loss: 0.272075
46,LA_T_3588714,0,46298,0, 111/118, Time: 0.551577s, Loss: 0.272075
3,LA_T_1373588,0,46949,0, 112/118, Time: 0.551577s, Loss: 0.272075
87,LA_T_6960076,0,42296,0, 113/118, Time: 0.551577s, Loss: 0.272075
20,LA_T_2269038,0,63442,0, 114/118, Time: 0.551577s, Loss: 0.272075
84,LA_T_6822716,0,27313,0, 115/118, Time: 0.551577s, Loss: 0.272075
70,LA_T_5883755,0,64000,0, 116/118, Time: 0.551577s, Loss: 0.272075
10,LA_T_1630611,0,58109,0, 117/118, Time: 0.551577s, Loss: 0.272075
32,LA_T_2995069,0,49112,0, 118/118, Time: 0.551577s, Loss: 0.272075
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Uni\FYP\Implementation\Code\SSL\core_scripts\data_io\customize_collate_fn.py:114: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = elem.storage()._new_shared(numel)
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Uni\FYP\Implementation\Code\SSL\core_scripts\data_io\customize_collate_fn.py:114: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = elem.storage()._new_shared(numel)
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
8,LA_D_1886178,0,40092,0, 1/63, Time: 0.254696s, Loss: 0.277379
19,LA_D_4013191,0,34443,0, 2/63, Time: 0.254696s, Loss: 0.277379
45,LA_D_7452217,0,25016,0, 3/63, Time: 0.254696s, Loss: 0.277379
55,LA_D_9192205,0,40612,0, 4/63, Time: 0.254696s, Loss: 0.277379
27,LA_D_5239066,1,30014,64000, 5/63, Time: 0.254696s, Loss: 0.277379
54,LA_D_8998984,0,37498,0, 6/63, Time: 0.254696s, Loss: 0.277379
33,LA_D_5659407,0,64000,0, 7/63, Time: 0.254696s, Loss: 0.277379
39,LA_D_5944972,1,39788,64000, 8/63, Time: 0.254696s, Loss: 0.277379
26,LA_D_5239066,0,64000,0, 9/63, Time: 0.254696s, Loss: 0.277379
0,LA_D_1179848,0,64000,0, 10/63, Time: 0.254696s, Loss: 0.277379
60,LA_D_9686838,1,28544,64000, 11/63, Time: 0.254696s, Loss: 0.277379
15,LA_D_3203408,0,49607,0, 12/63, Time: 0.254696s, Loss: 0.277379
50,LA_D_7974256,0,50504,0, 13/63, Time: 0.254696s, Loss: 0.277379
20,LA_D_4265541,0,26061,0, 14/63, Time: 0.254696s, Loss: 0.277379
28,LA_D_5300881,0,64000,0, 15/63, Time: 0.254696s, Loss: 0.277379
58,LA_D_9566347,0,35349,0, 16/63, Time: 0.254696s, Loss: 0.277379
57,LA_D_9493396,0,40904,0, 17/63, Time: 0.254696s, Loss: 0.277379
34,LA_D_5741681,0,44990,0, 18/63, Time: 0.254696s, Loss: 0.277379
48,LA_D_7869978,1,20551,64000, 19/63, Time: 0.254696s, Loss: 0.277379
49,LA_D_7933136,0,64000,0, 20/63, Time: 0.254696s, Loss: 0.277379
14,LA_D_2949136,1,27301,64000, 21/63, Time: 0.254696s, Loss: 0.277379
9,LA_D_2082512,0,53318,0, 22/63, Time: 0.254696s, Loss: 0.277379
4,LA_D_1366945,0,45986,0, 23/63, Time: 0.254696s, Loss: 0.277379
53,LA_D_8584336,0,61424,0, 24/63, Time: 0.254696s, Loss: 0.277379
13,LA_D_2949136,0,64000,0, 25/63, Time: 0.254696s, Loss: 0.277379
47,LA_D_7869978,0,64000,0, 26/63, Time: 0.254696s, Loss: 0.277379
21,LA_D_4276413,0,49755,0, 27/63, Time: 0.254696s, Loss: 0.277379
41,LA_D_6180779,0,23109,0, 28/63, Time: 0.254696s, Loss: 0.277379
36,LA_D_5835948,1,29523,64000, 29/63, Time: 0.254696s, Loss: 0.277379
3,LA_D_1317561,1,8976,64000, 30/63, Time: 0.254696s, Loss: 0.277379
11,LA_D_2648941,1,41414,64000, 31/63, Time: 0.254696s, Loss: 0.277379
17,LA_D_3705418,0,64000,0, 32/63, Time: 0.254696s, Loss: 0.277379
5,LA_D_1435765,0,47544,0, 33/63, Time: 0.254696s, Loss: 0.277379
51,LA_D_8228250,0,42344,0, 34/63, Time: 0.254696s, Loss: 0.277379
38,LA_D_5944972,0,64000,0, 35/63, Time: 0.254696s, Loss: 0.277379
42,LA_D_6502788,0,29166,0, 36/63, Time: 0.254696s, Loss: 0.277379
32,LA_D_5505879,1,35600,64000, 37/63, Time: 0.254696s, Loss: 0.277379
29,LA_D_5300881,1,27489,64000, 38/63, Time: 0.254696s, Loss: 0.277379
18,LA_D_3983088,0,21649,0, 39/63, Time: 0.254696s, Loss: 0.277379
12,LA_D_2896709,0,64000,0, 40/63, Time: 0.254696s, Loss: 0.277379
56,LA_D_9316963,0,58699,0, 41/63, Time: 0.254696s, Loss: 0.277379
10,LA_D_2648941,0,64000,0, 42/63, Time: 0.254696s, Loss: 0.277379
37,LA_D_5891869,0,54006,0, 43/63, Time: 0.254696s, Loss: 0.277379
2,LA_D_1317561,0,64000,0, 44/63, Time: 0.254696s, Loss: 0.277379
46,LA_D_7862876,0,57311,0, 45/63, Time: 0.254696s, Loss: 0.277379
6,LA_D_1580841,0,38838,0, 46/63, Time: 0.254696s, Loss: 0.277379
7,LA_D_1803008,0,45707,0, 47/63, Time: 0.254696s, Loss: 0.277379
61,LA_D_9753761,0,64000,0, 48/63, Time: 0.254696s, Loss: 0.277379
40,LA_D_6055606,0,53176,0, 49/63, Time: 0.254696s, Loss: 0.277379
23,LA_D_4519635,0,64000,0, 50/63, Time: 0.254696s, Loss: 0.277379
44,LA_D_7095518,0,56482,0, 51/63, Time: 0.254696s, Loss: 0.277379
35,LA_D_5835948,0,64000,0, 52/63, Time: 0.254696s, Loss: 0.277379
59,LA_D_9686838,0,64000,0, 53/63, Time: 0.254696s, Loss: 0.277379
1,LA_D_1179848,1,29541,64000, 54/63, Time: 0.254696s, Loss: 0.277379
62,LA_D_9753761,1,22997,64000, 55/63, Time: 0.254696s, Loss: 0.277379
43,LA_D_7026375,0,61915,0, 56/63, Time: 0.254696s, Loss: 0.277379
25,LA_D_4630224,0,27639,0, 57/63, Time: 0.254696s, Loss: 0.277379
31,LA_D_5505879,0,64000,0, 58/63, Time: 0.254696s, Loss: 0.277379
52,LA_D_8284460,0,50174,0, 59/63, Time: 0.254696s, Loss: 0.277379
16,LA_D_3457616,0,51111,0, 60/63, Time: 0.254696s, Loss: 0.277379
22,LA_D_4394367,0,61722,0, 61/63, Time: 0.254696s, Loss: 0.277379
30,LA_D_5441528,0,24518,0, 62/63, Time: 0.254696s, Loss: 0.277379
24,LA_D_4519635,1,12487,64000, 63/63, Time: 0.254696s, Loss: 0.277379
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Uni\FYP\Implementation\Code\SSL\core_scripts\data_io\customize_collate_fn.py:114: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = elem.storage()._new_shared(numel)
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
87,LA_T_6960076,0,42296,0, 1/118, Time: 0.489410s, Loss: 0.348849
32,LA_T_2995069,0,49112,0, 2/118, Time: 0.489410s, Loss: 0.348849
44,LA_T_3527643,0,64000,0, 3/118, Time: 0.489410s, Loss: 0.348849
58,LA_T_4928920,0,64000,0, 4/118, Time: 0.489410s, Loss: 0.348849
94,LA_T_7281115,1,11690,64000, 5/118, Time: 0.489410s, Loss: 0.348849
63,LA_T_5214047,0,41920,0, 6/118, Time: 0.489410s, Loss: 0.348849
7,LA_T_1518499,0,29284,0, 7/118, Time: 0.489410s, Loss: 0.348849
106,LA_T_8090857,0,64000,0, 8/118, Time: 0.489410s, Loss: 0.348849
49,LA_T_3758169,0,46047,0, 9/118, Time: 0.489410s, Loss: 0.348849
28,LA_T_2419035,1,10625,64000, 10/118, Time: 0.489410s, Loss: 0.348849
4,LA_T_1470918,0,39641,0, 11/118, Time: 0.489410s, Loss: 0.348849
43,LA_T_3467377,1,26131,64000, 12/118, Time: 0.489410s, Loss: 0.348849
50,LA_T_3935691,0,39156,0, 13/118, Time: 0.489410s, Loss: 0.348849
79,LA_T_6476207,0,54948,0, 14/118, Time: 0.489410s, Loss: 0.348849
21,LA_T_2277153,0,64000,0, 15/118, Time: 0.489410s, Loss: 0.348849
97,LA_T_7405543,0,36914,0, 16/118, Time: 0.489410s, Loss: 0.348849
9,LA_T_1589546,1,9736,64000, 17/118, Time: 0.489410s, Loss: 0.348849
27,LA_T_2419035,0,64000,0, 18/118, Time: 0.489410s, Loss: 0.348849
56,LA_T_4725126,0,61141,0, 19/118, Time: 0.489410s, Loss: 0.348849
62,LA_T_5209704,1,34511,64000, 20/118, Time: 0.489410s, Loss: 0.348849
60,LA_T_5205025,0,61550,0, 21/118, Time: 0.489410s, Loss: 0.348849
2,LA_T_1346935,0,45741,0, 22/118, Time: 0.489410s, Loss: 0.348849
18,LA_T_1909651,2,24481,128000, 23/118, Time: 0.489410s, Loss: 0.348849
29,LA_T_2422854,0,62661,0, 24/118, Time: 0.489410s, Loss: 0.348849
99,LA_T_7529623,0,43309,0, 25/118, Time: 0.489410s, Loss: 0.348849
11,LA_T_1653822,0,64000,0, 26/118, Time: 0.489410s, Loss: 0.348849
19,LA_T_2052267,0,38415,0, 27/118, Time: 0.489410s, Loss: 0.348849
80,LA_T_6487227,0,44111,0, 28/118, Time: 0.489410s, Loss: 0.348849
117,LA_T_9830298,0,64000,0, 29/118, Time: 0.489410s, Loss: 0.348849
34,LA_T_3021659,1,10227,64000, 30/118, Time: 0.489410s, Loss: 0.348849
101,LA_T_7706110,0,24083,0, 31/118, Time: 0.489410s, Loss: 0.348849
40,LA_T_3319380,0,42032,0, 32/118, Time: 0.489410s, Loss: 0.348849
95,LA_T_7314513,0,39437,0, 33/118, Time: 0.489410s, Loss: 0.348849
51,LA_T_3993688,0,48709,0, 34/118, Time: 0.489410s, Loss: 0.348849
42,LA_T_3467377,0,64000,0, 35/118, Time: 0.489410s, Loss: 0.348849
59,LA_T_5015679,0,41144,0, 36/118, Time: 0.489410s, Loss: 0.348849
36,LA_T_3189628,0,64000,0, 37/118, Time: 0.489410s, Loss: 0.348849
86,LA_T_6941395,1,29307,64000, 38/118, Time: 0.489410s, Loss: 0.348849
47,LA_T_3627265,0,64000,0, 39/118, Time: 0.489410s, Loss: 0.348849
6,LA_T_1490244,0,55572,0, 40/118, Time: 0.489410s, Loss: 0.348849
26,LA_T_2354581,0,47687,0, 41/118, Time: 0.489410s, Loss: 0.348849
31,LA_T_2677562,0,63649,0, 42/118, Time: 0.489410s, Loss: 0.348849
12,LA_T_1653822,1,9756,64000, 43/118, Time: 0.489410s, Loss: 0.348849
14,LA_T_1836557,0,43646,0, 44/118, Time: 0.489410s, Loss: 0.348849
96,LA_T_7359423,0,50164,0, 45/118, Time: 0.489410s, Loss: 0.348849
81,LA_T_6521388,0,64000,0, 46/118, Time: 0.489410s, Loss: 0.348849
54,LA_T_4584407,0,36921,0, 47/118, Time: 0.489410s, Loss: 0.348849
39,LA_T_3250806,0,61271,0, 48/118, Time: 0.489410s, Loss: 0.348849
5,LA_T_1486451,0,50406,0, 49/118, Time: 0.489410s, Loss: 0.348849
69,LA_T_5786445,0,64000,0, 50/118, Time: 0.489410s, Loss: 0.348849
67,LA_T_5492534,0,28365,0, 51/118, Time: 0.489410s, Loss: 0.348849
15,LA_T_1870524,0,43190,0, 52/118, Time: 0.489410s, Loss: 0.348849
68,LA_T_5522733,0,61381,0, 53/118, Time: 0.489410s, Loss: 0.348849
33,LA_T_3021659,0,64000,0, 54/118, Time: 0.489410s, Loss: 0.348849
16,LA_T_1909651,0,64000,0, 55/118, Time: 0.489410s, Loss: 0.348849
85,LA_T_6941395,0,64000,0, 56/118, Time: 0.489410s, Loss: 0.348849
46,LA_T_3588714,0,46298,0, 57/118, Time: 0.489410s, Loss: 0.348849
75,LA_T_6331069,0,64000,0, 58/118, Time: 0.489410s, Loss: 0.348849
83,LA_T_6751630,0,62290,0, 59/118, Time: 0.489410s, Loss: 0.348849
89,LA_T_7124444,0,64000,0, 60/118, Time: 0.489410s, Loss: 0.348849
22,LA_T_2277153,1,21822,64000, 61/118, Time: 0.489410s, Loss: 0.348849
3,LA_T_1373588,0,46949,0, 62/118, Time: 0.489410s, Loss: 0.348849
105,LA_T_8007992,0,45465,0, 63/118, Time: 0.489410s, Loss: 0.348849
76,LA_T_6331069,1,64000,64000, 64/118, Time: 0.489410s, Loss: 0.348849
61,LA_T_5209704,0,64000,0, 65/118, Time: 0.545200s, Loss: 0.325865
24,LA_T_2327393,0,60701,0, 66/118, Time: 0.545200s, Loss: 0.325865
17,LA_T_1909651,1,64000,64000, 67/118, Time: 0.545200s, Loss: 0.325865
57,LA_T_4774044,0,59379,0, 68/118, Time: 0.545200s, Loss: 0.325865
98,LA_T_7422011,0,45646,0, 69/118, Time: 0.545200s, Loss: 0.325865
70,LA_T_5883755,0,64000,0, 70/118, Time: 0.545200s, Loss: 0.325865
91,LA_T_7223887,0,40210,0, 71/118, Time: 0.545200s, Loss: 0.325865
35,LA_T_3176741,0,62198,0, 72/118, Time: 0.545200s, Loss: 0.325865
37,LA_T_3189628,1,15851,64000, 73/118, Time: 0.545200s, Loss: 0.325865
109,LA_T_8527420,0,63377,0, 74/118, Time: 0.545200s, Loss: 0.325865
104,LA_T_7888797,0,26626,0, 75/118, Time: 0.545200s, Loss: 0.325865
10,LA_T_1630611,0,58109,0, 76/118, Time: 0.545200s, Loss: 0.325865
110,LA_T_8681550,0,40836,0, 77/118, Time: 0.545200s, Loss: 0.325865
20,LA_T_2269038,0,63442,0, 78/118, Time: 0.545200s, Loss: 0.325865
1,LA_T_1258641,0,44562,0, 79/118, Time: 0.545200s, Loss: 0.325865
103,LA_T_7866363,0,63473,0, 80/118, Time: 0.545200s, Loss: 0.325865
100,LA_T_7550620,0,44981,0, 81/118, Time: 0.545200s, Loss: 0.325865
108,LA_T_8234484,0,46183,0, 82/118, Time: 0.545200s, Loss: 0.325865
25,LA_T_2333843,0,61708,0, 83/118, Time: 0.545200s, Loss: 0.325865
115,LA_T_9746209,0,64000,0, 84/118, Time: 0.545200s, Loss: 0.325865
102,LA_T_7808689,0,64000,0, 85/118, Time: 0.545200s, Loss: 0.325865
82,LA_T_6728875,0,64000,0, 86/118, Time: 0.545200s, Loss: 0.325865
13,LA_T_1724943,0,60906,0, 87/118, Time: 0.545200s, Loss: 0.325865
65,LA_T_5360018,0,54554,0, 88/118, Time: 0.545200s, Loss: 0.325865
78,LA_T_6435787,0,43614,0, 89/118, Time: 0.545200s, Loss: 0.325865
77,LA_T_6390981,0,64000,0, 90/118, Time: 0.545200s, Loss: 0.325865
90,LA_T_7124444,1,21405,64000, 91/118, Time: 0.545200s, Loss: 0.325865
52,LA_T_4096754,0,31273,0, 92/118, Time: 0.545200s, Loss: 0.325865
93,LA_T_7281115,0,64000,0, 93/118, Time: 0.545200s, Loss: 0.325865
55,LA_T_4630359,0,64000,0, 94/118, Time: 0.545200s, Loss: 0.325865
84,LA_T_6822716,0,27313,0, 95/118, Time: 0.545200s, Loss: 0.325865
116,LA_T_9746209,1,16967,64000, 96/118, Time: 0.545200s, Loss: 0.325865
23,LA_T_2320617,0,56846,0, 97/118, Time: 0.545200s, Loss: 0.325865
113,LA_T_9633872,0,27407,0, 98/118, Time: 0.545200s, Loss: 0.325865
114,LA_T_9668514,0,54564,0, 99/118, Time: 0.545200s, Loss: 0.325865
53,LA_T_4512109,0,51563,0, 100/118, Time: 0.545200s, Loss: 0.325865
88,LA_T_7020532,0,59863,0, 101/118, Time: 0.545200s, Loss: 0.325865
64,LA_T_5258800,0,54370,0, 102/118, Time: 0.545200s, Loss: 0.325865
111,LA_T_9226984,0,45529,0, 103/118, Time: 0.545200s, Loss: 0.325865
45,LA_T_3527643,1,8295,64000, 104/118, Time: 0.545200s, Loss: 0.325865
71,LA_T_5883755,1,19926,64000, 105/118, Time: 0.545200s, Loss: 0.325865
73,LA_T_5978739,0,33363,0, 106/118, Time: 0.545200s, Loss: 0.325865
8,LA_T_1589546,0,64000,0, 107/118, Time: 0.545200s, Loss: 0.325865
107,LA_T_8206162,0,55328,0, 108/118, Time: 0.545200s, Loss: 0.325865
74,LA_T_6301739,0,41843,0, 109/118, Time: 0.545200s, Loss: 0.325865
66,LA_T_5433188,0,34172,0, 110/118, Time: 0.545200s, Loss: 0.325865
38,LA_T_3220265,0,38149,0, 111/118, Time: 0.545200s, Loss: 0.325865
112,LA_T_9540683,0,25430,0, 112/118, Time: 0.545200s, Loss: 0.325865
92,LA_T_7273305,0,53173,0, 113/118, Time: 0.545200s, Loss: 0.325865
48,LA_T_3627265,1,44402,64000, 114/118, Time: 0.545200s, Loss: 0.325865
0,LA_T_1154440,0,30107,0, 115/118, Time: 0.545200s, Loss: 0.325865
72,LA_T_5894355,0,64000,0, 116/118, Time: 0.545200s, Loss: 0.325865
41,LA_T_3391018,0,57402,0, 117/118, Time: 0.545200s, Loss: 0.325865
30,LA_T_2584761,0,47107,0, 118/118, Time: 0.545200s, Loss: 0.325865
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Uni\FYP\Implementation\Code\SSL\core_scripts\data_io\customize_collate_fn.py:114: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = elem.storage()._new_shared(numel)
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Uni\FYP\Implementation\Code\SSL\core_scripts\data_io\customize_collate_fn.py:114: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = elem.storage()._new_shared(numel)
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
48,LA_D_7869978,1,20551,64000, 1/63, Time: 0.243905s, Loss: 0.281539
43,LA_D_7026375,0,61915,0, 2/63, Time: 0.243905s, Loss: 0.281539
36,LA_D_5835948,1,29523,64000, 3/63, Time: 0.243905s, Loss: 0.281539
14,LA_D_2949136,1,27301,64000, 4/63, Time: 0.243905s, Loss: 0.281539
60,LA_D_9686838,1,28544,64000, 5/63, Time: 0.243905s, Loss: 0.281539
24,LA_D_4519635,1,12487,64000, 6/63, Time: 0.243905s, Loss: 0.281539
35,LA_D_5835948,0,64000,0, 7/63, Time: 0.243905s, Loss: 0.281539
46,LA_D_7862876,0,57311,0, 8/63, Time: 0.243905s, Loss: 0.281539
32,LA_D_5505879,1,35600,64000, 9/63, Time: 0.243905s, Loss: 0.281539
13,LA_D_2949136,0,64000,0, 10/63, Time: 0.243905s, Loss: 0.281539
1,LA_D_1179848,1,29541,64000, 11/63, Time: 0.243905s, Loss: 0.281539
26,LA_D_5239066,0,64000,0, 12/63, Time: 0.243905s, Loss: 0.281539
45,LA_D_7452217,0,25016,0, 13/63, Time: 0.243905s, Loss: 0.281539
28,LA_D_5300881,0,64000,0, 14/63, Time: 0.243905s, Loss: 0.281539
15,LA_D_3203408,0,49607,0, 15/63, Time: 0.243905s, Loss: 0.281539
34,LA_D_5741681,0,44990,0, 16/63, Time: 0.243905s, Loss: 0.281539
11,LA_D_2648941,1,41414,64000, 17/63, Time: 0.243905s, Loss: 0.281539
40,LA_D_6055606,0,53176,0, 18/63, Time: 0.243905s, Loss: 0.281539
31,LA_D_5505879,0,64000,0, 19/63, Time: 0.243905s, Loss: 0.281539
16,LA_D_3457616,0,51111,0, 20/63, Time: 0.243905s, Loss: 0.281539
42,LA_D_6502788,0,29166,0, 21/63, Time: 0.243905s, Loss: 0.281539
18,LA_D_3983088,0,21649,0, 22/63, Time: 0.243905s, Loss: 0.281539
27,LA_D_5239066,1,30014,64000, 23/63, Time: 0.243905s, Loss: 0.281539
38,LA_D_5944972,0,64000,0, 24/63, Time: 0.243905s, Loss: 0.281539
30,LA_D_5441528,0,24518,0, 25/63, Time: 0.243905s, Loss: 0.281539
51,LA_D_8228250,0,42344,0, 26/63, Time: 0.243905s, Loss: 0.281539
23,LA_D_4519635,0,64000,0, 27/63, Time: 0.243905s, Loss: 0.281539
49,LA_D_7933136,0,64000,0, 28/63, Time: 0.243905s, Loss: 0.281539
50,LA_D_7974256,0,50504,0, 29/63, Time: 0.243905s, Loss: 0.281539
57,LA_D_9493396,0,40904,0, 30/63, Time: 0.243905s, Loss: 0.281539
5,LA_D_1435765,0,47544,0, 31/63, Time: 0.243905s, Loss: 0.281539
22,LA_D_4394367,0,61722,0, 32/63, Time: 0.243905s, Loss: 0.281539
41,LA_D_6180779,0,23109,0, 33/63, Time: 0.243905s, Loss: 0.281539
53,LA_D_8584336,0,61424,0, 34/63, Time: 0.243905s, Loss: 0.281539
56,LA_D_9316963,0,58699,0, 35/63, Time: 0.243905s, Loss: 0.281539
61,LA_D_9753761,0,64000,0, 36/63, Time: 0.243905s, Loss: 0.281539
10,LA_D_2648941,0,64000,0, 37/63, Time: 0.243905s, Loss: 0.281539
8,LA_D_1886178,0,40092,0, 38/63, Time: 0.243905s, Loss: 0.281539
17,LA_D_3705418,0,64000,0, 39/63, Time: 0.243905s, Loss: 0.281539
39,LA_D_5944972,1,39788,64000, 40/63, Time: 0.243905s, Loss: 0.281539
7,LA_D_1803008,0,45707,0, 41/63, Time: 0.243905s, Loss: 0.281539
25,LA_D_4630224,0,27639,0, 42/63, Time: 0.243905s, Loss: 0.281539
44,LA_D_7095518,0,56482,0, 43/63, Time: 0.243905s, Loss: 0.281539
0,LA_D_1179848,0,64000,0, 44/63, Time: 0.243905s, Loss: 0.281539
12,LA_D_2896709,0,64000,0, 45/63, Time: 0.243905s, Loss: 0.281539
20,LA_D_4265541,0,26061,0, 46/63, Time: 0.243905s, Loss: 0.281539
6,LA_D_1580841,0,38838,0, 47/63, Time: 0.243905s, Loss: 0.281539
3,LA_D_1317561,1,8976,64000, 48/63, Time: 0.243905s, Loss: 0.281539
37,LA_D_5891869,0,54006,0, 49/63, Time: 0.243905s, Loss: 0.281539
4,LA_D_1366945,0,45986,0, 50/63, Time: 0.243905s, Loss: 0.281539
62,LA_D_9753761,1,22997,64000, 51/63, Time: 0.243905s, Loss: 0.281539
2,LA_D_1317561,0,64000,0, 52/63, Time: 0.243905s, Loss: 0.281539
21,LA_D_4276413,0,49755,0, 53/63, Time: 0.243905s, Loss: 0.281539
58,LA_D_9566347,0,35349,0, 54/63, Time: 0.243905s, Loss: 0.281539
47,LA_D_7869978,0,64000,0, 55/63, Time: 0.243905s, Loss: 0.281539
54,LA_D_8998984,0,37498,0, 56/63, Time: 0.243905s, Loss: 0.281539
52,LA_D_8284460,0,50174,0, 57/63, Time: 0.243905s, Loss: 0.281539
33,LA_D_5659407,0,64000,0, 58/63, Time: 0.243905s, Loss: 0.281539
29,LA_D_5300881,1,27489,64000, 59/63, Time: 0.243905s, Loss: 0.281539
55,LA_D_9192205,0,40612,0, 60/63, Time: 0.243905s, Loss: 0.281539
59,LA_D_9686838,0,64000,0, 61/63, Time: 0.243905s, Loss: 0.281539
9,LA_D_2082512,0,53318,0, 62/63, Time: 0.243905s, Loss: 0.281539
19,LA_D_4013191,0,34443,0, 63/63, Time: 0.243905s, Loss: 0.281539
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Uni\FYP\Implementation\Code\SSL\core_scripts\data_io\customize_collate_fn.py:114: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  storage = elem.storage()._new_shared(numel)
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "D:\Software\Anaconda\envs\fairseq-pip2\lib\multiprocessing\spawn.py", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File "D:\Software\Anaconda\envs\fairseq-pip2\lib\multiprocessing\spawn.py", line 125, in _main
    prepare(preparation_data)
  File "D:\Software\Anaconda\envs\fairseq-pip2\lib\multiprocessing\spawn.py", line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "D:\Software\Anaconda\envs\fairseq-pip2\lib\multiprocessing\spawn.py", line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File "D:\Software\Anaconda\envs\fairseq-pip2\lib\runpy.py", line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "D:\Software\Anaconda\envs\fairseq-pip2\lib\runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "D:\Software\Anaconda\envs\fairseq-pip2\lib\runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "D:\Uni\FYP\Implementation\Code\SSL\project\model-WavLM\config_train_toyset\02\main.py", line 4, in <module>
    import torch
  File "D:\Software\Anaconda\envs\fairseq-pip2\lib\site-packages\torch\__init__.py", line 125, in <module>
    res = kernel32.LoadLibraryExW(dll, None, 0x00001100)
KeyboardInterrupt
